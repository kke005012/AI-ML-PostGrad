{"cells":[{"cell_type":"markdown","id":"k-xVqOB6QTm6","metadata":{"id":"k-xVqOB6QTm6"},"source":["# **Problem Statement**"]},{"cell_type":"markdown","id":"AJDbTeirQasg","metadata":{"id":"AJDbTeirQasg"},"source":["## Business Context"]},{"cell_type":"markdown","id":"94bMSnpWQmuA","metadata":{"id":"94bMSnpWQmuA"},"source":["A sales forecast is a prediction of future sales revenue based on historical data, industry trends, and the status of the current sales pipeline. Businesses use the sales forecast to estimate weekly, monthly, quarterly, and annual sales totals. A company needs to make an accurate sales forecast as it adds value across an organization and helps the different verticals to chalk out their future course of action.\n","\n","Forecasting helps an organization plan its sales operations by region and provides valuable insights to the supply chain team regarding the procurement of goods and materials. An accurate sales forecast process has many benefits which include improved decision-making about the future and reduction of sales pipeline and forecast risks. Moreover, it helps to reduce the time spent in planning territory coverage and establish benchmarks that can be used to assess trends in the future."]},{"cell_type":"markdown","id":"Aasy7LC_Qpq5","metadata":{"id":"Aasy7LC_Qpq5"},"source":["## Objective"]},{"cell_type":"markdown","id":"khshBslaQtX9","metadata":{"id":"khshBslaQtX9"},"source":["SuperKart is a retail chain operating supermarkets and food marts across various tier cities, offering a wide range of products. To optimize its inventory management and make informed decisions around regional sales strategies, SuperKart wants to accurately forecast the sales revenue of its outlets for the upcoming quarter.\n","\n","To operationalize these insights at scale, the company has partnered with a data science firmâ€”not just to build a predictive model based on historical sales data, but to develop and deploy a robust forecasting solution that can be integrated into SuperKartâ€™s decision-making systems and used across its network of stores."]},{"cell_type":"markdown","id":"v-HxlIhTQ0-E","metadata":{"id":"v-HxlIhTQ0-E"},"source":["## Data Description"]},{"cell_type":"markdown","id":"c0670116","metadata":{"id":"c0670116"},"source":["The data contains the different attributes of the various products and stores.The detailed data dictionary is given below.\n","\n","- **Product_Id** - unique identifier of each product, each identifier having two letters at the beginning followed by a number.\n","- **Product_Weight** - weight of each product\n","- **Product_Sugar_Content** - sugar content of each product like low sugar, regular and no sugar\n","- **Product_Allocated_Area** - ratio of the allocated display area of each product to the total display area of all the products in a store\n","- **Product_Type** - broad category for each product like meat, snack foods, hard drinks, dairy, canned, soft drinks, health and hygiene, baking goods, bread, breakfast, frozen foods, fruits and vegetables, household, seafood, starchy foods, others\n","- **Product_MRP** - maximum retail price of each product\n","- **Store_Id** - unique identifier of each store\n","- **Store_Establishment_Year** - year in which the store was established\n","- **Store_Size** - size of the store depending on sq. feet like high, medium and low\n","- **Store_Location_City_Type** - type of city in which the store is located like Tier 1, Tier 2 and Tier 3. Tier 1 consists of cities where the standard of living is comparatively higher than its Tier 2 and Tier 3 counterparts.\n","- **Store_Type** - type of store depending on the products that are being sold there like Departmental Store, Supermarket Type 1, Supermarket Type 2 and Food Mart\n","- **Product_Store_Sales_Total** - total revenue generated by the sale of that particular product in that particular store\n"]},{"cell_type":"markdown","id":"60VhOlydQ-PG","metadata":{"id":"60VhOlydQ-PG"},"source":["# **Installing and Importing the necessary libraries**"]},{"cell_type":"code","execution_count":null,"id":"yisDjKOB6TqF","metadata":{"id":"yisDjKOB6TqF"},"outputs":[],"source":["#Installing the libraries with the specified versions\n","!pip install numpy==2.0.2 pandas==2.2.2 scikit-learn==1.6.1 matplotlib==3.10.0 seaborn==0.13.2 joblib==1.4.2 xgboost==2.1.4 requests==2.32.3 huggingface_hub==0.30.1 -q"]},{"cell_type":"markdown","id":"m-wZg1XZ6bLa","metadata":{"id":"m-wZg1XZ6bLa"},"source":["**Note:**\n","\n","- After running the above cell, kindly restart the notebook kernel (for Jupyter Notebook) or runtime (for Google Colab) and run all cells sequentially from the next cell.\n","\n","- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook."]},{"cell_type":"code","execution_count":null,"id":"c0022e4d","metadata":{"id":"c0022e4d"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Libraries to help with reading and manipulating data\n","import numpy as np\n","import pandas as pd\n","\n","# For splitting the dataset\n","from sklearn.model_selection import train_test_split\n","\n","# Libaries to help with data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Removes the limit for the number of displayed columns\n","pd.set_option(\"display.max_columns\", None)\n","# Sets the limit for the number of displayed rows\n","pd.set_option(\"display.max_rows\", 100)\n","\n","\n","# Libraries different ensemble classifiers\n","from sklearn.ensemble import (\n","    BaggingRegressor,\n","    RandomForestRegressor,\n","    AdaBoostRegressor,\n","    GradientBoostingRegressor,\n",")\n","from xgboost import XGBRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","\n","# Libraries to get different metric scores\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    mean_squared_error,\n","    mean_absolute_error,\n","    r2_score,\n","    mean_absolute_percentage_error\n",")\n","\n","# To create the pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.compose import make_column_transformer\n","from sklearn.pipeline import make_pipeline,Pipeline\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","# To tune different models and standardize\n","from sklearn.model_selection import GridSearchCV\n","\n","# To serialize the model\n","import joblib\n","\n","# os related functionalities\n","import os\n","\n","# API request\n","from flask import Flask, request, jsonify\n","\n","# for hugging face space authentication to upload files\n","from huggingface_hub import login, HfApi"]},{"cell_type":"markdown","id":"51b91836","metadata":{"id":"51b91836"},"source":["# **Loading the dataset**"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"8mlhsnvDtQcP"},"id":"8mlhsnvDtQcP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load data\n","df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Module 7 - Deployment/Project - SuperKart/SuperKart.csv\")"],"metadata":{"id":"YckcdzTlp839"},"id":"YckcdzTlp839","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"W2sXwrUERYua","metadata":{"id":"W2sXwrUERYua"},"source":["# **Data Overview**"]},{"cell_type":"code","source":["# Shape of dataset\n","print(\"Shape of dataset:\", df.shape)\n","\n","# Data types of each column\n","print(\"\\nData Types:\")\n","print(df.dtypes)\n","\n","# Missing values\n","print(\"\\nMissing Values:\")\n","print(df.isnull().sum())\n","\n","# Duplicate rows\n","print(\"\\nDuplicate Rows:\", df.duplicated().sum())\n","\n","# Statistical summary - numeric + categorical\n","print(\"\\nSummary Statistics (numeric & categorical):\")\n","display(df.describe(include=\"all\").transpose())"],"metadata":{"id":"rtomleOyp8dQ"},"id":"rtomleOyp8dQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Iterate through columns and print unique values\n","for column in df.columns:\n","    unique_values = df[column].unique()\n","    print(f\"Unique values in column '{column}': {unique_values}\")"],"metadata":{"id":"CpbGQ72VHZsj"},"id":"CpbGQ72VHZsj","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace the value 'reg' with 'Regular' in the specified column\n","df['Product_Sugar_Content'] = df['Product_Sugar_Content'].replace('reg', 'Regular')"],"metadata":{"id":"g4i2fwFlIAfh"},"id":"g4i2fwFlIAfh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Iterate through columns and print unique values to do sanity check on replacement of reg to Regular\n","for column in df.columns:\n","    unique_values = df[column].unique()\n","    print(f\"Unique values in column '{column}': {unique_values}\")"],"metadata":{"id":"k_TTXSRnIZqB"},"id":"k_TTXSRnIZqB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Data Overview Observations\n","\n","* Dataset shape: 8,763 rows x 12 columns â†’ medium-sized\n","dataset, manageable for tree-based models.\n","\n","* Target variable: Product_Store_Sales_Total represents revenue per product per store â†’ continuous numeric variable suitable for regression.\n","\n","* Missing values: None detected across all columns â†’ no imputation needed.\n","\n","* Duplicates: None found â†’ dataset is clean and unique at the row level.\n","\n","* Replace: Unique values for Product_Sugar_Content included reg and Regular.  Replaced reg with Regular for consistency.\n","\n","* Product features:\n","  - Product_Id is unique per product, not directly useful for modeling â†’ can be dropped or used for grouping only.\n","\n","  - Product_Weight ranges 4-22 (avg ~12.6) â†’ fairly consistent weights with no missing values.\n","\n","  - Product_Sugar_Content has 4 categories, with â€œLow Sugarâ€ dominating (~55%).\n","\n","  - Product_Type has 16 categories, most frequent = Fruits & Vegetables (~14%).\n","\n","  - Product_Allocated_Area ranges 0.004-0.298 (avg ~0.069) â†’ reflects shelf space share.\n","\n","   - Product_MRP ranges 31-266 (avg ~147) â†’ moderately wide distribution.\n","\n","* Store features:\n","\n","  - Store_Id: only 4 unique stores in dataset â†’ important grouping factor.\n","\n","  - Store_Establishment_Year: ranges 1987-2009 â†’ store age can be engineered as a new feature.\n","\n","  - Store_Size: 3 levels (High/Medium/Low), Medium most common (~69%).\n","\n","  - Store_Location_City_Type: Tier 1-3, Tier 2 dominates (~71%).\n","\n","  - Store_Type: 4 categories (Departmental, Supermarket Type 1 & 2, Food Mart).\n","\n","* Overall quality: Dataset is clean, balanced, with no missing/duplicate issues. Rich mix of numeric and categorical variables well-suited for tree-based regression models."],"metadata":{"id":"JgloyIaJtjov"},"id":"JgloyIaJtjov"},{"cell_type":"markdown","id":"4oamAwxrVHLq","metadata":{"id":"4oamAwxrVHLq"},"source":["# **Exploratory Data Analysis (EDA)**"]},{"cell_type":"markdown","source":["## Univariate Analysis"],"metadata":{"id":"y1gvMTV9qCa1"},"id":"y1gvMTV9qCa1"},{"cell_type":"markdown","source":["**Goal:** Understand the distribution and spread of each key feature."],"metadata":{"id":"4GabFh2Rttxw"},"id":"4GabFh2Rttxw"},{"cell_type":"code","source":["# --- Univariate Analysis: Numerical Variables ---\n","numeric_cols = [\"Product_Weight\", \"Product_Allocated_Area\", \"Product_MRP\", \"Product_Store_Sales_Total\"]\n","\n","for col in numeric_cols:\n","    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","    # Histogram + KDE\n","    sns.histplot(df[col], kde=True, ax=axes[0], bins=30, color=\"skyblue\")\n","    axes[0].set_title(f\"Distribution of {col}\")\n","\n","    # Boxplot\n","    sns.boxplot(x=df[col], ax=axes[1], color=\"lightgreen\")\n","    axes[1].set_title(f\"Boxplot of {col}\")\n","\n","    plt.tight_layout()\n","    plt.show();"],"metadata":{"id":"g4dMNNsAqGy3"},"id":"g4dMNNsAqGy3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Univariate Analysis: Categorical Variables ---\n","cat_cols = [\"Product_Sugar_Content\", \"Product_Type\", \"Store_Size\",\n","            \"Store_Location_City_Type\", \"Store_Type\"]\n","\n","for col in cat_cols:\n","    plt.figure(figsize=(8, 4))\n","    order = df[col].value_counts().index  # sort by frequency\n","    sns.countplot(data=df, x=col, order=order, palette=\"viridis\")\n","    plt.title(f\"Count of {col}\")\n","    plt.xticks(rotation=45)\n","    plt.show();"],"metadata":{"id":"KUiWqj9qt35q"},"id":"KUiWqj9qt35q","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Univariate Observations\n","#### Numeric Features\n","* Product_Weight\n","  - Summary: mean â‰ˆ 12.65, median 12.66, min 4.0, max 22.0, std â‰ˆ 2.22.\n","\n","  - Distribution is essentially symmetric (skew â‰ˆ 0.02) and fairly tight â€” few extreme physical-weight values (weight looks well-behaved).\n","\n","* Product_Allocated_Area\n","  - Summary: mean â‰ˆ 0.0688, median 0.056, min 0.004, max 0.298, std â‰ˆ 0.048.\n","  - Clearly right-skewed (skew â‰ˆ 1.13). Most products occupy a small share of shelf area; a small number of products have much larger allocated area (these will appear as â€œoutliersâ€ on boxplots).\n","\n","* Product_MRP\n","  - Summary: mean â‰ˆ 147.03, median 146.74, min 31.0, max 266.0, std â‰ˆ 30.69.\n","  - Fairly symmetric (skew â‰ˆ 0.04) with a moderate spread â€” no obvious data-entry extremes, but there are high-priced products at the top end of the range.\n","\n","* Product_Store_Sales_Total (target)\n","  - Summary: mean â‰ˆ 3,464.00, median 3,452.34, min 33.0, max 8,000.0, std â‰ˆ 1,065.57.\n","  - Distribution is not heavily skewed (skew â‰ˆ 0.09), but there is a wide spread (33 â†’ 8,000). Boxplots will show many points beyond the whiskers; some are likely real high-value product-store sales rather than errors.\n","\n","* Some numeric columns (especially Product_Allocated_Area and the target) show many points beyond boxplot whiskers. That doesn't automatically mean they're bad â€” these large values may be real business signals (e.g., top sellers with big shelf share or very high revenue). Recommended approach: flag outliers (IQR or percentile), inspect those rows for business plausibility, then decide (no removal by default).\n","\n","#### Categorical features (key observations)\n","* Product_Sugar_Content â€” 4 distinct labels, but there's an inconsistent label: reg (108 rows) appears alongside Regular (2,251 rows).\n","  - Top = Low Sugar (4,885 rows â‰ˆ 55.8%) â†’ dataset dominated by Low Sugar products.\n","  - Action: normalize reg â†’ Regular before encoding.\n","\n","* Product_Type â€” 16 categories.\n","  - Top categories: Fruits and Vegetables (1,249), Snack Foods (1,149), Frozen Foods (811), etc.\n","  - A few product types account for a large share; long tail of less frequent categories.\n","\n","* Store_Id â€” only 4 unique stores.\n","  - Top store: OUT004 (4,676 rows â‰ˆ 53.4%) â€” over half the dataset comes from this single outlet.\n","  - Implication: store-level imbalance is important for validation (use grouped splits / GroupKFold by Store_Id so we don't leak store-specific patterns).\n","\n","* Store_Size â€” 3 levels; Medium is dominant (6,025 rows â‰ˆ 68.8%).\n","\n","* Store_Location_City_Type â€” 3 tiers; Tier 2 dominates (6,262 rows â‰ˆ 71.5%).\n","\n","* Store_Type â€” 4 types; Supermarket Type2 is the largest (4,676 rows â‰ˆ 53.4%) â€” same count as OUT004, indicating a close tie between that store and that type.\n","\n","* Implication of categorical imbalances: many categorical variables are heavily imbalanced (one store, one city tier, and one store size/type dominate). This affects model generalization and evaluation â€” validate on stores/tiers that are not over-represented."],"metadata":{"id":"6Qjm2cbvt-gc"},"id":"6Qjm2cbvt-gc"},{"cell_type":"markdown","source":[],"metadata":{"id":"4_2lBwUPt-et"},"id":"4_2lBwUPt-et"},{"cell_type":"markdown","source":["## Bivariate Analysis"],"metadata":{"id":"zk4dmiKGqEmr"},"id":"zk4dmiKGqEmr"},{"cell_type":"markdown","source":["**Goal**: See relationships between independent variables and sales."],"metadata":{"id":"if0kjBdzuE5m"},"id":"if0kjBdzuE5m"},{"cell_type":"code","source":["# --- Numeric vs Target ---\n","# Sales vs Product MRP\n","plt.figure(figsize=(6,4))\n","sns.scatterplot(x=\"Product_MRP\", y=\"Product_Store_Sales_Total\", data=df, alpha=0.5)\n","plt.title(\"Sales vs Product MRP\")\n","plt.show();"],"metadata":{"id":"V4I4kXz0qHQf"},"id":"V4I4kXz0qHQf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sales vs Product Weight\n","plt.figure(figsize=(6,4))\n","sns.scatterplot(x=\"Product_Weight\", y=\"Product_Store_Sales_Total\", data=df, alpha=0.5)\n","plt.title(\"Sales vs Product Weight\")\n","plt.show();"],"metadata":{"id":"WyqUiOBdqLSN"},"id":"WyqUiOBdqLSN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Categorical vs Target ---\n","# Average sales by sugar content\n","plt.figure(figsize=(6,4))\n","sns.boxplot(x=\"Product_Sugar_Content\", y=\"Product_Store_Sales_Total\", data=df)\n","plt.title(\"Sales by Sugar Content\")\n","plt.show();"],"metadata":{"id":"Qn2-kd5Qul5v"},"id":"Qn2-kd5Qul5v","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Average sales by product type\n","plt.figure(figsize=(10,5))\n","sns.boxplot(x=\"Product_Type\", y=\"Product_Store_Sales_Total\", data=df)\n","plt.xticks(rotation=45)\n","plt.title(\"Sales by Product Type\")\n","plt.show();"],"metadata":{"id":"u58mw2M6unpk"},"id":"u58mw2M6unpk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Average sales by store type\n","plt.figure(figsize=(6,4))\n","sns.boxplot(x=\"Store_Type\", y=\"Product_Store_Sales_Total\", data=df)\n","plt.xticks(rotation=45)\n","plt.title(\"Sales by Store Type\")\n","plt.show();"],"metadata":{"id":"722fde6vunoW"},"id":"722fde6vunoW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Bivariate Analysis Observations\n","\n","* Product MRP vs Sales: There is a positive trendâ€”products with higher MRP tend to generate more sales revenue, though variability increases with higher prices.\n","\n","* Product Weight vs Sales: No strong linear relationship is observed; however, heavier products show more variability in sales, suggesting product type and category might be stronger drivers.\n","\n","* Sugar Content vs Sales: Products with regular sugar content show higher and more consistent sales compared to â€œlow sugarâ€ or â€œno sugarâ€ variants, indicating consumer preference.\n","\n","* Product Type vs Sales: Certain product types (e.g., snack foods, dairy, and soft drinks) dominate in terms of sales revenue, while niche categories contribute less.\n","\n","* Store Type vs Sales: Supermarket Type 1 and Departmental Stores tend to generate higher sales than Food Marts, suggesting store format influences revenue potential."],"metadata":{"id":"LELw47fFuxgZ"},"id":"LELw47fFuxgZ"},{"cell_type":"markdown","source":["### Multivariate/Interactions\n","\n","**Goal:** Find combined effects.\n","\n"],"metadata":{"id":"vfVqMXRDuxdr"},"id":"vfVqMXRDuxdr"},{"cell_type":"code","source":["# Numeric features heatmap\n","plt.figure(figsize=(8,6))\n","corr = df[[\"Product_Weight\",\"Product_Allocated_Area\",\"Product_MRP\",\"Product_Store_Sales_Total\"]].corr()\n","sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n","plt.title(\"Correlation Heatmap (Numeric Features)\")\n","plt.show();"],"metadata":{"id":"Vacq2k57vBSC"},"id":"Vacq2k57vBSC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sales by Store Location and Store Size boxplot\n","plt.figure(figsize=(8,5))\n","sns.boxplot(x=\"Store_Location_City_Type\", y=\"Product_Store_Sales_Total\", hue=\"Store_Size\", data=df)\n","plt.title(\"Sales by City Type and Store Size\")\n","plt.show();"],"metadata":{"id":"J_u36wejvE76"},"id":"J_u36wejvE76","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Product Type & Store Type boxplot\n","plt.figure(figsize=(12,6))\n","sns.boxplot(x=\"Product_Type\", y=\"Product_Store_Sales_Total\", hue=\"Store_Type\", data=df)\n","plt.xticks(rotation=45)\n","plt.title(\"Sales by Product Type across Store Types\")\n","plt.show();"],"metadata":{"id":"xL8Z3pPJvIfg"},"id":"xL8Z3pPJvIfg","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multivariate Observations (based on expected patterns):\n","\n","* Correlation Heatmap:\n","  - Product_MRP has a moderate positive correlation with sales, while other numeric features (like weight and allocated area) show weaker relationships.\n","  - No severe multicollinearity observed among numeric predictors.\n","\n","* Store Location x Store Size:\n","  - Tier 1 cities with large stores tend to have the highest sales, highlighting the effect of both market affluence and store capacity.\n","  - Tier 3 cities with smaller stores have noticeably lower sales, suggesting regional and scale differences in customer spending.\n","\n","* Product Type x Store Type:\n","  - Categories like snack foods, dairy, and soft drinks perform strongly across all store types but show especially high sales in Supermarkets.\n","  - Niche categories (e.g., seafood, health & hygiene) have lower sales overall, but their contribution is slightly higher in Departmental Stores, suggesting format-specific customer demand."],"metadata":{"id":"XBkzc3WpvNNG"},"id":"XBkzc3WpvNNG"},{"cell_type":"markdown","id":"0fo5OvIfVdtB","metadata":{"id":"0fo5OvIfVdtB"},"source":["# **Data Preprocessing**"]},{"cell_type":"code","source":["# --- Missing Values Check ---\n","missing_report = df.isnull().sum()\n","missing_report = missing_report[missing_report > 0].sort_values(ascending=False)\n","\n","print(\"Missing Values per Column:\\n\")\n","print(missing_report)\n","\n","# Show rows with missing Product_Weight\n","print(\"\\nSample rows with missing Product_Weight:\")\n","display(df[df[\"Product_Weight\"].isnull()].head())\n","\n","# Show rows with missing Product_Allocated_Area\n","print(\"\\nSample rows with missing Product_Allocated_Area:\")\n","display(df[df[\"Product_Allocated_Area\"].isnull()].head())"],"metadata":{"id":"UUZdKUyWv0jX"},"id":"UUZdKUyWv0jX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Hidden Missing Values Check ---\n","\n","# 1. Empty strings\n","empty_counts = (df == \"\").sum()\n","\n","# 2. Common placeholders\n","placeholders = [\"NA\", \"N/A\", \"Missing\", \"Unknown\", \"null\", \"Null\", \"NONE\"]\n","placeholder_counts = df.isin(placeholders).sum()\n","\n","# 3. Negative or zero in numeric columns\n","numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n","negative_or_zero = (df[numeric_cols] <= 0).sum()\n","\n","print(\"Empty String Counts:\\n\", empty_counts[empty_counts > 0], \"\\n\")\n","print(\"Placeholder String Counts:\\n\", placeholder_counts[placeholder_counts > 0], \"\\n\")\n","print(\"Negative or Zero Values (possible missing encodings):\\n\", negative_or_zero[negative_or_zero > 0])"],"metadata":{"id":"8cwSpgJev7Om"},"id":"8cwSpgJev7Om","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Outlier Detection using IQR ---\n","\n","numeric_cols = [\"Product_Weight\", \"Product_Allocated_Area\",\n","                \"Product_MRP\", \"Product_Store_Sales_Total\"]\n","\n","outlier_summary = {}\n","\n","for col in numeric_cols:\n","    Q1 = df[col].quantile(0.25)\n","    Q3 = df[col].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower = Q1 - 1.5 * IQR\n","    upper = Q3 + 1.5 * IQR\n","\n","    outliers = df[(df[col] < lower) | (df[col] > upper)]\n","    outlier_summary[col] = {\n","        \"LowerBound\": lower,\n","        \"UpperBound\": upper,\n","        \"OutlierCount\": len(outliers),\n","        \"OutlierPercent\": round(len(outliers) / len(df) * 100, 2)\n","    }\n","\n","outlier_df = pd.DataFrame(outlier_summary).T\n","outlier_df"],"metadata":{"id":"sD9kpbuywG3F"},"id":"sD9kpbuywG3F","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Numeric features\n","num_cols = [\"Product_Weight\", \"Product_Allocated_Area\",\n","            \"Product_MRP\", \"Product_Store_Sales_Total\"]\n","\n","# Function to compute outlier stats\n","def outlier_summary(df, col):\n","    Q1 = df[col].quantile(0.25)\n","    Q3 = df[col].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower = Q1 - 1.5 * IQR\n","    upper = Q3 + 1.5 * IQR\n","    outliers = df[(df[col] < lower) | (df[col] > upper)][col]\n","    return {\n","        \"LowerBound\": round(lower, 4),\n","        \"UpperBound\": round(upper, 4),\n","        \"OutlierCount\": outliers.shape[0],\n","        \"OutlierPercent\": round(100 * outliers.shape[0] / df.shape[0], 2)\n","    }\n","\n","# Outlier summary table\n","outlier_results = pd.DataFrame(\n","    {col: outlier_summary(df, col) for col in num_cols}\n",").T.reset_index().rename(columns={\"index\": \"Feature\"})\n","\n","print(outlier_results)\n","\n","# Regular boxplots\n","plt.figure(figsize=(12, 8))\n","for i, col in enumerate(num_cols, 1):\n","    plt.subplot(2, 2, i)\n","    sns.boxplot(x=df[col], color=\"skyblue\", fliersize=3)\n","    plt.axvline(outlier_summary(df, col)[\"LowerBound\"], color='red', linestyle='--')\n","    plt.axvline(outlier_summary(df, col)[\"UpperBound\"], color='red', linestyle='--')\n","    plt.title(f\"Boxplot of {col}\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Log-transformed boxplots (only for skewed features)\n","log_cols = [\"Product_MRP\", \"Product_Store_Sales_Total\"]\n","\n","plt.figure(figsize=(10, 5))\n","for i, col in enumerate(log_cols, 1):\n","    plt.subplot(1, 2, i)\n","    sns.boxplot(x=np.log1p(df[col]), color=\"lightgreen\", fliersize=3)\n","    plt.title(f\"Log-Transformed Boxplot of {col}\")\n","plt.tight_layout()\n","plt.show();"],"metadata":{"id":"hDM_2K88wces"},"id":"hDM_2K88wces","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in [\"Product_MRP\", \"Product_Store_Sales_Total\"]:\n","    print(f\"{col} skewness before: {df[col].skew():.2f}\")\n","    print(f\"{col} skewness after log1p: {np.log1p(df[col]).skew():.2f}\")\n","    print(\"-\"*40)"],"metadata":{"id":"L3i_sYMiw6vs"},"id":"L3i_sYMiw6vs","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Observation:\n","Log transformation was tested on 'Product_MRP' and 'Product_Store_Sales_Total' to check for skewness reduction.  \n","However, both features already had near-zero skewness (â‰ˆ0), and applying 'log1p' introduced moderate to strong left skew.  \n","Therefore, the transformation was not applied in the final preprocessing pipeline.\n"],"metadata":{"id":"VVLV6slnxLFf"},"id":"VVLV6slnxLFf"},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Define target and features\n","X = df.drop(columns=[\"Product_Store_Sales_Total\"])\n","y = df[\"Product_Store_Sales_Total\"]\n","\n","# Train-test split (80-20)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","print(\"Train set shape:\", X_train.shape)\n","print(\"Test set shape:\", X_test.shape)"],"metadata":{"id":"FW9qonwKxwPO"},"id":"FW9qonwKxwPO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Identify column types ---\n","numeric_features = [\"Product_Weight\", \"Product_Allocated_Area\", \"Product_MRP\"]\n","categorical_features = [\n","    \"Product_Sugar_Content\",\n","    \"Product_Type\",\n","    \"Store_Size\",\n","    \"Store_Location_City_Type\",\n","    \"Store_Type\"\n","]\n","\n","# --- Preprocessing steps ---\n","numeric_transformer = Pipeline(steps=[\n","    (\"scaler\", StandardScaler())\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n","])\n","\n","# --- Full ColumnTransformer ---\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        (\"num\", numeric_transformer, numeric_features),\n","        (\"cat\", categorical_transformer, categorical_features)\n","    ]\n",")\n","\n","print(\"Preprocessing pipeline is ready!\")"],"metadata":{"id":"kz8sLMO0x4BN"},"id":"kz8sLMO0x4BN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Preprocessing Observations\n","\n","* Missing values: No explicit or hidden missing values (NaN, empty strings, or placeholder encodings) were detected. Hence, no imputation was required.\n","\n","* Duplicates: Dataset contained no duplicate rows, so no action was needed.\n","\n","* Outliers:\n","  - Outliers were detected in Product_Weight, Product_Allocated_Area, Product_MRP, and Product_Store_Sales_Total.\n","  - The proportion of outliers was small (< 2% per feature).\n","  - These values may represent genuine business cases (premium products, high-selling items, or large product weights). Hence, they were retained.\n","\n","* Skewness check:\n","  - Product_MRP and Product_Store_Sales_Total were tested with log transformation.\n","  - Both features already had near-zero skewness. Applying log1p introduced moderate-to-strong left skew, so the transformation was not applied.\n","* Train-Test Split: Data was split into training (80%) and testing (20%) sets with random_state=42 for reproducibility.\n","\n","* Preprocessing pipeline setup:\n","  - Numeric features: Product_Weight, Product_Allocated_Area, and Product_MRP â†’ standardized using StandardScaler.\n","  - Categorical features: Product_Sugar_Content, Product_Type, Store_Size, Store_Location_City_Type, and Store_Type â†’ encoded using OneHotEncoder with handle_unknown=\"ignore\".\n","  - All transformations were combined into a ColumnTransformer, which will be integrated into the ML model pipelines to ensure consistency during training, evaluation, and deployment."],"metadata":{"id":"c-_TKkj7yVLF"},"id":"c-_TKkj7yVLF"},{"cell_type":"markdown","id":"5fd3cabe","metadata":{"id":"5fd3cabe"},"source":["# **Model Building**"]},{"cell_type":"markdown","id":"YyzOQ8pBY93N","metadata":{"id":"YyzOQ8pBY93N"},"source":["## Define functions for Model Evaluation"]},{"cell_type":"code","execution_count":null,"id":"d107c3d3","metadata":{"id":"d107c3d3"},"outputs":[],"source":["# function to compute adjusted R-squared\n","def adj_r2_score(predictors, targets, predictions):\n","    r2 = r2_score(targets, predictions)\n","    n = predictors.shape[0]\n","    k = predictors.shape[1]\n","    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n","\n","\n","# function to compute different metrics to check performance of a regression model\n","def model_performance_regression(model, predictors, target):\n","    \"\"\"\n","    Function to compute different metrics to check regression model performance\n","\n","    model: regressor\n","    predictors: independent variables\n","    target: dependent variable\n","    \"\"\"\n","\n","    # predicting using the independent variables\n","    pred = model.predict(predictors)\n","\n","    r2 = r2_score(target, pred)  # to compute R-squared\n","    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared\n","    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE\n","    mae = mean_absolute_error(target, pred)  # to compute MAE\n","    mape = mean_absolute_percentage_error(target, pred)  # to compute MAPE\n","\n","    # creating a dataframe of metrics\n","    df_perf = pd.DataFrame(\n","        {\n","            \"RMSE\": rmse,\n","            \"MAE\": mae,\n","            \"R-squared\": r2,\n","            \"Adj. R-squared\": adjr2,\n","            \"MAPE\": mape,\n","        },\n","        index=[0],\n","    )\n","\n","    return df_perf"]},{"cell_type":"markdown","source":["The ML models to be built can be any two out of the following:\n","1. Decision Tree\n","2. Bagging\n","3. Random Forest\n","4. AdaBoost\n","5. Gradient Boosting\n","6. XGBoost"],"metadata":{"id":"P14pbR8nAefF"},"id":"P14pbR8nAefF"},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from xgboost import XGBRegressor\n","\n","# --- Random Forest Pipeline ---\n","rf_model = Pipeline(steps=[\n","    (\"preprocessor\", preprocessor),\n","    (\"regressor\", RandomForestRegressor(random_state=42, n_jobs=-1))\n","])\n","\n","# Fit on training data\n","rf_model.fit(X_train, y_train)\n","\n","# Evaluate\n","rf_train_perf = model_performance_regression(rf_model, X_train, y_train)\n","rf_test_perf = model_performance_regression(rf_model, X_test, y_test)\n","\n","print(\"Random Forest - Train Performance:\\n\", rf_train_perf)\n","print(\"Random Forest - Test Performance:\\n\", rf_test_perf)\n","\n","\n","# --- XGBoost Pipeline ---\n","xgb_model = Pipeline(steps=[\n","    (\"preprocessor\", preprocessor),\n","    (\"regressor\", XGBRegressor(random_state=42, n_jobs=-1, verbosity=0))\n","])\n","\n","# Fit on training data\n","xgb_model.fit(X_train, y_train)\n","\n","# Evaluate\n","xgb_train_perf = model_performance_regression(xgb_model, X_train, y_train)\n","xgb_test_perf = model_performance_regression(xgb_model, X_test, y_test)\n","\n","print(\"\\nXGBoost - Train Performance:\\n\", xgb_train_perf)\n","print(\"XGBoost - Test Performance:\\n\", xgb_test_perf)"],"metadata":{"id":"plU8dqWjAd9b"},"id":"plU8dqWjAd9b","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine results into one dataframe to use metrics to find best model and help with fine-tuning\n","results = pd.concat([\n","    rf_train_perf.assign(Model=\"Random Forest\", Dataset=\"Train\"),\n","    rf_test_perf.assign(Model=\"Random Forest\", Dataset=\"Test\"),\n","    xgb_train_perf.assign(Model=\"XGBoost\", Dataset=\"Train\"),\n","    xgb_test_perf.assign(Model=\"XGBoost\", Dataset=\"Test\")\n","])\n","\n","# Reset index before melting\n","results = results.reset_index(drop=True)\n","\n","# Now make long format for plotting\n","results_long = results.melt(id_vars=[\"Model\", \"Dataset\"],\n","                            var_name=\"Metric\",\n","                            value_name=\"Score\")"],"metadata":{"id":"kkVARKmE1lRe"},"id":"kkVARKmE1lRe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the trained Random Forest and XGBoost estimators from the pipelines\n","rf_estimator = rf_model.named_steps[\"regressor\"]\n","xgb_estimator = xgb_model.named_steps[\"regressor\"]\n","\n","# Extract feature names after preprocessing\n","feature_names = preprocessor.get_feature_names_out()\n","\n","# Random Forest importance\n","rf_importance = pd.DataFrame({\n","    \"Feature\": feature_names,\n","    \"Importance\": rf_estimator.feature_importances_\n","}).sort_values(by=\"Importance\", ascending=False)\n","\n","# XGBoost importance\n","xgb_importance = pd.DataFrame({\n","    \"Feature\": feature_names,\n","    \"Importance\": xgb_estimator.feature_importances_\n","}).sort_values(by=\"Importance\", ascending=False)\n","\n","# Plot\n","plt.figure(figsize=(8,5))\n","sns.barplot(data=rf_importance.head(10), x=\"Importance\", y=\"Feature\", palette=\"viridis\")\n","plt.title(\"Top 10 Important Features - Random Forest\")\n","plt.show();\n","\n","plt.figure(figsize=(8,5))\n","sns.barplot(data=xgb_importance.head(10), x=\"Importance\", y=\"Feature\", palette=\"plasma\")\n","plt.title(\"Top 10 Important Features - XGBoost\")\n","plt.show();"],"metadata":{"id":"GiUuILYB2_E9"},"id":"GiUuILYB2_E9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Feature Importance Observations\n","\n","* Random Forest identified Product_MRP (Maximum Retail Price) as the most important feature, confirming the expectation that product pricing strongly influences total store sales.\n","\n","* XGBoost, while still recognizing product-level drivers, gave higher importance to store-related features. This indicates that store characteristics (size, location, allocation, type) significantly affect sales outcomes.\n","\n","* The difference in rankings highlights how Random Forest captures broad, dominant relationships (e.g., price â†’ sales), whereas XGBoost emphasizes interaction effects and residual variability (e.g., store amplifying/reducing the effect of price).\n","\n","* Business takeaway:\n","  - Pricing strategy is critical for driving sales.\n","  - Optimizing store-specific factors (allocation area, store type) can enhance performance beyond price alone."],"metadata":{"id":"5L4vftXW3Y_M"},"id":"5L4vftXW3Y_M"},{"cell_type":"markdown","id":"FtkIDTjdYy5h","metadata":{"id":"FtkIDTjdYy5h"},"source":["# **Model Performance Improvement - Hyperparameter Tuning**"]},{"cell_type":"code","source":["# ---------------------------\n","# Adjusted RÂ² helper\n","# ---------------------------\n","def adj_r2_score(predictors, targets, predictions):\n","    r2 = r2_score(targets, predictions)\n","    n = predictors.shape[0]\n","    k = predictors.shape[1]\n","    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n","\n","\n","# ---------------------------\n","# Evaluation function\n","# ---------------------------\n","def evaluate_model(model, X_train, y_train, X_test, y_test, model_name=\"Model\"):\n","    metrics = {}\n","\n","    for split, X, y in [(\"Train\", X_train, y_train), (\"Test\", X_test, y_test)]:\n","        preds = model.predict(X)\n","        rmse = np.sqrt(mean_squared_error(y, preds))\n","        mae = mean_absolute_error(y, preds)\n","        r2 = r2_score(y, preds)\n","        adjr2 = adj_r2_score(X, y, preds)\n","        mape = mean_absolute_percentage_error(y, preds)\n","\n","        metrics[f\"{split} RMSE\"] = rmse\n","        metrics[f\"{split} MAE\"] = mae\n","        metrics[f\"{split} RÂ²\"] = r2\n","        metrics[f\"{split} Adj. RÂ²\"] = adjr2\n","        metrics[f\"{split} MAPE\"] = mape\n","\n","    return pd.DataFrame(metrics, index=[model_name])\n","\n","\n","# ---------------------------\n","# Random Forest GridSearchCV\n","# ---------------------------\n","rf_param_grid = {\n","    \"regressor__n_estimators\": [100, 200],\n","    \"regressor__max_depth\": [10, 20, None],\n","    \"regressor__min_samples_split\": [2, 5],\n","    \"regressor__min_samples_leaf\": [1, 2],\n","    \"regressor__max_features\": [\"sqrt\", \"log2\"]\n","}\n","\n","grid_search_rf = GridSearchCV(\n","    rf_model,\n","    param_grid=rf_param_grid,\n","    cv=3,\n","    scoring=\"r2\",\n","    n_jobs=-1,\n","    verbose=2\n",")\n","\n","grid_search_rf.fit(X_train, y_train)\n","best_rf = grid_search_rf.best_estimator_\n","print(\"Best RF Params:\", grid_search_rf.best_params_)\n","\n","\n","# ---------------------------\n","# XGBoost GridSearchCV\n","# ---------------------------\n","xgb_param_grid = {\n","    \"regressor__n_estimators\": [100, 200],\n","    \"regressor__max_depth\": [3, 6, 10],\n","    \"regressor__learning_rate\": [0.01, 0.1, 0.2],\n","    \"regressor__subsample\": [0.8, 1.0],\n","    \"regressor__colsample_bytree\": [0.8, 1.0]\n","}\n","\n","grid_search_xgb = GridSearchCV(\n","    xgb_model,\n","    param_grid=xgb_param_grid,\n","    cv=3,\n","    scoring=\"r2\",\n","    n_jobs=-1,\n","    verbose=2\n",")\n","\n","grid_search_xgb.fit(X_train, y_train)\n","best_xgb = grid_search_xgb.best_estimator_\n","print(\"Best XGB Params:\", grid_search_xgb.best_params_)\n","\n","\n","# ---------------------------\n","# Compare all models\n","# ---------------------------\n","results = pd.concat([\n","    evaluate_model(rf_model, X_train, y_train, X_test, y_test, \"Random Forest (Baseline)\"),\n","    evaluate_model(best_rf, X_train, y_train, X_test, y_test, \"Random Forest (Tuned)\"),\n","    evaluate_model(xgb_model, X_train, y_train, X_test, y_test, \"XGBoost (Baseline)\"),\n","    evaluate_model(best_xgb, X_train, y_train, X_test, y_test, \"XGBoost (Tuned)\")\n","])\n","\n","print(results)"],"metadata":{"id":"0Q5GpNDA8FEW"},"id":"0Q5GpNDA8FEW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert wide results table into long format\n","results_long = results.reset_index().melt(id_vars=\"index\", var_name=\"Metric\", value_name=\"Score\")\n","results_long.rename(columns={\"index\": \"Model\"}, inplace=True)\n","\n","plt.figure(figsize=(12,6))\n","heatmap = sns.heatmap(\n","    results_long.pivot(index=\"Model\", columns=\"Metric\", values=\"Score\"),\n","    annot=True, fmt=\".3f\", cmap=\"YlGnBu\", cbar=True\n",")\n","\n","plt.title(\"Model Performance Comparison (Train vs Test)\", fontsize=14, pad=15)\n","plt.ylabel(\"Model\")\n","plt.xlabel(\"Metric\")\n","plt.xticks(rotation=30)\n","plt.show();"],"metadata":{"id":"HvxyJGLGAeng"},"id":"HvxyJGLGAeng","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set plot style\n","sns.set(style=\"whitegrid\", context=\"talk\")\n","\n","# Extract 'Train' or 'Test' from the 'Metric' column to create a 'Dataset' column\n","results_long[\"Dataset\"] = results_long[\"Metric\"].apply(lambda x: \"Train\" if \"Train\" in x else \"Test\")\n","# Remove 'Train ' or 'Test ' from the 'Metric' column for cleaner labels\n","results_long[\"Metric\"] = results_long[\"Metric\"].str.replace(\"Train \", \"\").str.replace(\"Test \", \"\")\n","\n","\n","# Create barplot\n","plt.figure(figsize=(12, 6))\n","sns.barplot(\n","    data=results_long,\n","    x=\"Metric\",\n","    y=\"Score\",\n","    hue=\"Dataset\",\n","    palette=\"Set2\",\n","    ci=None\n",")\n","\n","# Add facet for Model (one subplot per model)\n","g = sns.catplot(\n","    data=results_long,\n","    x=\"Metric\",\n","    y=\"Score\",\n","    hue=\"Dataset\",\n","    col=\"Model\",\n","    kind=\"bar\",\n","    palette=\"Set2\",\n","    height=6,\n","    aspect=1\n",")\n","\n","g.set_titles(\"{col_name}\")  # Show just model name\n","g.set_axis_labels(\"Metric\", \"Score\")\n","for ax in g.axes.flat:\n","    for p in ax.patches:\n","        ax.annotate(f\"{p.get_height():.2f}\",\n","                    (p.get_x() + p.get_width() / 2., p.get_height()),\n","                    ha=\"center\", va=\"bottom\", fontsize=9, color=\"black\", rotation=0)\n","\n","plt.show();"],"metadata":{"id":"GxW7ODotFuBR"},"id":"GxW7ODotFuBR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a clean comparison table\n","comparison_table = results_long.pivot(index=\"Model\", columns=[\"Metric\", \"Dataset\"], values=\"Score\")\n","comparison_table = comparison_table.sort_index(axis=1, level=0)\n","\n","# Function to highlight best score per metric (higher is better for RÂ², lower is better for others)\n","def highlight_best(s):\n","    is_best = s == s.max() if s.name[0] in [\"RÂ²\", \"Adj. RÂ²\"] else s == s.min()\n","    return ['font-weight: bold' if v else '' for v in is_best]\n","\n","# Apply styling\n","styled_table = comparison_table.style.apply(highlight_best, axis=0)\n","\n","\n","print(\"ðŸ“Š Model Performance Comparison (Train vs Test)\\n\")\n","display(styled_table.format(precision=3))"],"metadata":{"id":"x32s6j2sGffT"},"id":"x32s6j2sGffT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Convert wide results table into long format for plotting\n","# Reset index to make 'Model' a column, then melt\n","results_long = results.reset_index().melt(\n","    id_vars=\"index\",  # Use the reset index (model names) as id_vars\n","    var_name=\"Metric_Dataset\", # Temporary column name for original column names\n","    value_name=\"Score\"\n",")\n","\n","# Rename the 'index' column to 'Model'\n","results_long.rename(columns={\"index\": \"Model\"}, inplace=True)\n","\n","# Extract 'Train' or 'Test' and the cleaned metric name from 'Metric_Dataset'\n","results_long[\"Dataset\"] = results_long[\"Metric_Dataset\"].apply(lambda x: \"Train\" if \"Train\" in x else \"Test\")\n","results_long[\"Metric\"] = results_long[\"Metric_Dataset\"].str.replace(\"Train \", \"\").str.replace(\"Test \", \"\")\n","\n","# Drop the temporary column\n","results_long.drop(columns=[\"Metric_Dataset\"], inplace=True)\n","\n","\n","# Create barplot\n","plt.figure(figsize=(12, 6))\n","sns.barplot(\n","    data=results_long,\n","    x=\"Metric\",\n","    y=\"Score\",\n","    hue=\"Model\", # Use Model as hue to compare models side-by-side\n","    palette=\"Set2\",\n","    ci=None # Set ci to None to avoid confidence intervals\n",")\n","\n","plt.title(\"Model Performance Comparison\", fontsize=14, weight=\"bold\")\n","plt.xlabel(\"Metric\", fontsize=12)\n","plt.ylabel(\"Score\", fontsize=12)\n","plt.xticks(rotation=30, ha=\"right\")\n","plt.legend(title=\"Model\")\n","plt.tight_layout()\n","plt.show();"],"metadata":{"id":"ZQIQriC-G-wF"},"id":"ZQIQriC-G-wF","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"b0810287","metadata":{"id":"b0810287"},"source":["# **Model Performance Comparison, Final Model Selection, and Serialization**"]},{"cell_type":"code","source":["# Prepare long format for plotting\n","results_long = results.reset_index().melt(\n","    id_vars=\"index\", # Use the reset index (model names) as id_vars\n","    var_name=\"Metric_Dataset\", # Temporary column name for original column names\n","    value_name=\"Score\"\n",")\n","\n","# Rename the 'index' column to 'Model'\n","results_long.rename(columns={\"index\": \"Model\"}, inplace=True)\n","\n","# Extract 'Train' or 'Test' and the cleaned metric name from 'Metric_Dataset'\n","results_long[\"Dataset\"] = results_long[\"Metric_Dataset\"].apply(lambda x: \"Train\" if \"Train\" in x else \"Test\")\n","results_long[\"Metric\"] = results_long[\"Metric_Dataset\"].str.replace(\"Train \", \"\").str.replace(\"Test \", \"\")\n","\n","# Drop the temporary column\n","results_long.drop(columns=[\"Metric_Dataset\"], inplace=True)\n","\n","# Create subplots: Train vs Test\n","fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=False)\n","\n","# Plot Train metrics\n","sns.barplot(\n","    data=results_long[results_long[\"Dataset\"] == \"Train\"],\n","    x=\"Metric\",\n","    y=\"Score\",\n","    hue=\"Model\",\n","    palette=\"Set2\",\n","    ci=None,\n","    ax=axes[0]\n",")\n","axes[0].set_title(\"Train Performance\", fontsize=14, weight=\"bold\")\n","axes[0].set_xlabel(\"Metric\", fontsize=12)\n","axes[0].set_ylabel(\"Score\", fontsize=12)\n","axes[0].tick_params(axis=\"x\", rotation=30)\n","\n","# Plot Test metrics\n","sns.barplot(\n","    data=results_long[results_long[\"Dataset\"] == \"Test\"],\n","    x=\"Metric\",\n","    y=\"Score\",\n","    hue=\"Model\",\n","    palette=\"Set2\",\n","    ci=None,\n","    ax=axes[1]\n",")\n","axes[1].set_title(\"Test Performance\", fontsize=14, weight=\"bold\")\n","axes[1].set_xlabel(\"Metric\", fontsize=12)\n","axes[1].set_ylabel(\"Score\", fontsize=12)\n","axes[1].tick_params(axis=\"x\", rotation=30)\n","\n","plt.tight_layout()\n","plt.show();"],"metadata":{"id":"X2M3eYFgqZEk"},"id":"X2M3eYFgqZEk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics = results_long[\"Metric\"].unique()\n","n_metrics = len(metrics)\n","\n","fig, axes = plt.subplots(1, n_metrics, figsize=(5*n_metrics, 5), sharey=False)\n","\n","for i, metric in enumerate(metrics):\n","    sns.barplot(\n","        data=results_long[results_long[\"Metric\"] == metric],\n","        x=\"Dataset\",\n","        y=\"Score\",\n","        hue=\"Model\",\n","        palette=\"Set2\",\n","        ci=None,\n","        ax=axes[i]\n","    )\n","    axes[i].set_title(metric, fontsize=14, weight=\"bold\")\n","    axes[i].set_xlabel(\"Dataset\")\n","    axes[i].set_ylabel(\"Score\")\n","\n","plt.tight_layout()\n","plt.show();"],"metadata":{"id":"ghsCzZNnHc4C"},"id":"ghsCzZNnHc4C","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create heatmap after converting wide to long\n","\n","data = {\n","    \"Model\": [\"Random Forest\", \"XGBoost\"],\n","    \"Test_Adj. R-squared\": [0.929, 0.916],\n","    \"Test_MAE\": [108.507, 136.766],\n","    \"Test_MAPE\": [0.039, 0.051],\n","    \"Test_R-squared\": [0.930, 0.916],\n","    \"Test_RMSE\": [282.956, 308.773],\n","    \"Train_Adj. R-squared\": [0.990, 0.984],\n","    \"Train_MAE\": [40.269, 63.904],\n","    \"Train_MAPE\": [0.015, 0.023],\n","    \"Train_R-squared\": [0.990, 0.984],\n","    \"Train_RMSE\": [106.999, 133.464],\n","}\n","df_metrics = pd.DataFrame(data)\n","\n","# Convert wide â†’ long\n","df_long = df_metrics.melt(id_vars=\"Model\", var_name=\"Dataset_Metric\", value_name=\"Score\")\n","\n","# Split into Dataset (Train/Test) and Metric\n","df_long[[\"Dataset\", \"Metric\"]] = df_long[\"Dataset_Metric\"].str.split(\"_\", n=1, expand=True)\n","\n","# Pivot for heatmap\n","heatmap_data = df_long.pivot_table(\n","    index=[\"Model\", \"Dataset\"],\n","    columns=\"Metric\",\n","    values=\"Score\"\n",")\n","\n","# Plot heatmap\n","plt.figure(figsize=(14, 6))\n","sns.heatmap(\n","    heatmap_data,\n","    annot=True, fmt=\".3f\", cmap=\"YlGnBu\", cbar=True\n",")\n","\n","plt.title(\"Model Performance Heatmap\", fontsize=16, weight=\"bold\")\n","plt.ylabel(\"Model / Dataset\", fontsize=12)\n","plt.xlabel(\"Metric\", fontsize=12)\n","plt.xticks(rotation=30)   # rotate x-axis labels\n","plt.yticks(rotation=0)    # keep model/dataset labels horizontal\n","plt.tight_layout()\n","plt.show();"],"metadata":{"id":"TIfXbSH-H_g7"},"id":"TIfXbSH-H_g7","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Final Model Selection Observations\n","\n","* Between the two candidate models (Random Forest and XGBoost), Random Forest consistently outperforms XGBoost across all test set metrics (RÂ², Adjusted RÂ², RMSE, MAE, and MAPE).\n","\n","* Random Forest Test RÂ² = 0.93, indicating that the model explains ~93% of the variance in sales on unseen data, compared to ~91.6% for XGBoost.\n","\n","* Error metrics (RMSE, MAE, and MAPE) are notably lower for Random Forest, suggesting stronger predictive accuracy and robustness.\n","\n","* Both models show a gap between training and testing performance (Train RÂ² â‰ˆ 0.99 vs. Test RÂ² â‰ˆ 0.93 for Random Forest), but the generalization remains strong without severe overfitting.\n","\n","* XGBoost performed reasonably well, but its test performance lagged behind Random Forest despite comparable training accuracy, suggesting less efficient generalization.\n","\n","* Based on this analysis, Random Forest is selected as the final model for predicting store sales in the SuperKart dataset.\n","\n","* Further hyperparameter tuning on Random Forest (e.g., adjusting max_features, min_samples_split, or bootstrap strategy) could provide marginal improvements, but the current model already demonstrates strong performance and generalizability."],"metadata":{"id":"Utpk_zgpImrX"},"id":"Utpk_zgpImrX"},{"cell_type":"code","source":["# Save the trained Random Forest pipeline (best estimator from GridSearchCV)\n","joblib.dump(best_rf, \"superkart_pipeline.pkl\")\n","\n","print(\"âœ… Random Forest pipeline saved as superkart_pipeline.pkl\")"],"metadata":{"id":"gLkxWHZ8hkLN"},"id":"gLkxWHZ8hkLN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load and test serialized model\n","rf_model_loaded = joblib.load(\"superkart_pipeline.pkl\")\n","\n","# Predict on the test set\n","y_pred = rf_model_loaded.predict(X_test)\n","\n","# Evaluate performance again\n","r2 = r2_score(y_test, y_pred)\n","rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","mae = mean_absolute_error(y_test, y_pred)\n","mape = mean_absolute_percentage_error(y_test, y_pred)\n","\n","print(\"ðŸ” Verification on Test Set\")\n","print(f\"R-squared: {r2:.3f}\")\n","print(f\"RMSE: {rmse:.3f}\")\n","print(f\"MAE: {mae:.3f}\")\n","print(f\"MAPE: {mape:.3f}\")"],"metadata":{"id":"2eJgrE5FLAXQ"},"id":"2eJgrE5FLAXQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sanity check before deployment\n","import pandas as pd\n","\n","# Pick one row from X_test (with raw categorical + numeric features)\n","sample = X_test.iloc[[0]]  # keep it as DataFrame\n","\n","print(\"Sample row from X_test:\")\n","print(sample)\n","\n","# Predict using the best pipeline\n","try:\n","    pred = best_rf.predict(sample)\n","    print(\"\\nâœ… Prediction works! Output:\", pred)\n","except Exception as e:\n","    print(\"\\nâŒ Prediction failed:\", e)"],"metadata":{"id":"BzyV5JxJ3Tex"},"id":"BzyV5JxJ3Tex","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"9a2LCguV-7i1","metadata":{"id":"9a2LCguV-7i1"},"source":["# **Deployment - Backend**"]},{"cell_type":"code","source":["# Create a folder for storing the files needed for backend server deployment\n","import os\n","os.makedirs(\"backend_files\", exist_ok=True)"],"metadata":{"id":"UdMo7uRJohH-"},"id":"UdMo7uRJohH-","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"T3XlDPUtJnDo","metadata":{"id":"T3XlDPUtJnDo"},"source":["## Flask Web Framework\n"]},{"cell_type":"code","source":["pip install Flask-CORS"],"metadata":{"id":"IrS6Ppo1rNdX"},"id":"IrS6Ppo1rNdX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile backend_files/app.py\n","\n","# backend_files/app.py\n","from flask import Flask, request, jsonify\n","from flask_cors import CORS\n","import pandas as pd\n","import joblib\n","import os\n","\n","app = Flask(\"Super Kart Sales Forecasting\")\n","CORS(app)\n","\n","model_path = \"superkart_pipeline.pkl\"\n","try:\n","    model = joblib.load(model_path)\n","except Exception as e:\n","    print(f\"âŒ Failed to load model: {e}\")\n","    model = None\n","\n","if hasattr(model.named_steps[\"preprocessor\"], \"transformers_\"):\n","    required_fields = []\n","    for name, trans, cols in model.named_steps[\"preprocessor\"].transformers_:\n","        if isinstance(cols, list):\n","            required_fields.extend(cols)\n","        elif isinstance(cols, str):\n","            required_fields.append(cols)\n","    REQUIRED_FIELDS = list(set(required_fields))\n","else:\n","    REQUIRED_FIELDS = []\n","\n","print(\"âœ… Required fields:\", REQUIRED_FIELDS)\n","\n","NUMERIC_FIELDS = [\n","    \"Product_Allocated_Area\",\n","    \"Product_MRP\",\n","    \"Product_Weight\"\n","]\n","CATEGORICAL_FIELDS = [f for f in REQUIRED_FIELDS if f not in NUMERIC_FIELDS]\n","\n","\n","def validate_record(record):\n","    if not isinstance(record, dict):\n","        return False, \"Each record must be a JSON object.\"\n","    missing = [f for f in REQUIRED_FIELDS if f not in record]\n","    if missing:\n","        return False, f\"Missing required fields: {', '.join(missing)}\"\n","    return True, None\n","\n","\n","def prepare_dataframe(data):\n","    if isinstance(data, dict):\n","        df = pd.DataFrame([data])\n","    elif isinstance(data, list) and all(isinstance(rec, dict) for rec in data):\n","        df = pd.DataFrame(data)\n","    else:\n","        raise ValueError(\"Input must be a dict or list of dicts.\")\n","\n","    for field in REQUIRED_FIELDS:\n","        if field not in df.columns:\n","            df[field] = None\n","\n","    for field in NUMERIC_FIELDS:\n","        if field in df.columns:\n","            df[field] = pd.to_numeric(df[field], errors=\"coerce\").fillna(0)\n","\n","    for field in CATEGORICAL_FIELDS:\n","        if field in df.columns:\n","            df[field] = df[field].astype(str).fillna(\"missing\")\n","\n","    df = df[REQUIRED_FIELDS]\n","\n","    return df\n","\n","\n","@app.route(\"/\")\n","def home():\n","    return \"âœ… SuperKart Sales Forecast API is running!\"\n","\n","\n","@app.route(\"/schema\", methods=[\"GET\"])\n","def schema():\n","    return jsonify({\n","        \"required_fields\": REQUIRED_FIELDS,\n","        \"example_record\": {field: \"value_here\" for field in REQUIRED_FIELDS}\n","    })\n","\n","\n","@app.route(\"/predict\", methods=[\"POST\"])\n","def predict():\n","    try:\n","        data = request.get_json(force=True)\n","\n","        valid, error = validate_record(data)\n","        if not valid:\n","            return jsonify({\"error\": error}), 400\n","\n","        input_df = prepare_dataframe(data)\n","\n","        # Debug print\n","        print(\"ðŸ” /predict received DataFrame:\")\n","        print(input_df.dtypes)\n","        print(input_df.head())\n","\n","        prediction = model.predict(input_df)\n","\n","        return jsonify({\"prediction\": float(prediction[0])})\n","\n","    except Exception as e:\n","        return jsonify({\"error\": str(e)}), 400\n","\n","\n","@app.route(\"/batch_predict\", methods=[\"POST\"])\n","def batch_predict():\n","    try:\n","        data = request.get_json(force=True)\n","\n","        if not isinstance(data, list):\n","            return jsonify({\"error\": \"Input must be a list of JSON records\"}), 400\n","\n","        for i, record in enumerate(data):\n","            valid, error = validate_record(record)\n","            if not valid:\n","                return jsonify({\"error\": f\"Record {i}: {error}\"}), 400\n","\n","        input_df = prepare_dataframe(data)\n","\n","        # Debug print\n","        print(\"ðŸ” /batch_predict received DataFrame:\")\n","        print(input_df.dtypes)\n","        print(input_df.head())\n","\n","        predictions = model.predict(input_df)\n","\n","        return jsonify({\"predictions\": predictions.tolist()})\n","\n","    except Exception as e:\n","        return jsonify({\"error\": str(e)}), 400\n","\n","\n","if __name__ == \"__main__\":\n","    app.run(debug=True, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", 7860)))"],"metadata":{"id":"D_ibq6X2qkZ0"},"id":"D_ibq6X2qkZ0","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"STDSb04iT-rL","metadata":{"id":"STDSb04iT-rL"},"source":["## Dependencies File"]},{"cell_type":"code","source":["%%writefile backend_files/requirements.txt\n","\n","flask==2.3.3\n","flask_cors==3.0.10\n","pandas==2.2.3\n","scikit-learn==1.5.2\n","joblib==1.4.2\n","numpy==1.26.4\n","gunicorn==23.0.0"],"metadata":{"id":"tZbPjRESqlf2"},"id":"tZbPjRESqlf2","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"JWD7rPCRUEtD","metadata":{"id":"JWD7rPCRUEtD"},"source":["## Dockerfile"]},{"cell_type":"code","source":["%%writefile backend_files/Dockerfile\n","\n","# Use official lightweight Python image\n","FROM python:3.12-slim\n","\n","# Set working directory\n","WORKDIR /app\n","\n","# Copy requirements.txt and install dependencies\n","COPY . .\n","\n","# Install dependencies from the requirements.txt file\n","RUN pip install --no-cache-dir -r requirements.txt\n","\n","# Expose the port Flask will run on\n","EXPOSE 7860\n","\n","# Hugging Face Spaces - app to bind to 0.0.0.0 and port 7860\n","ENV PORT=7860\n","ENV HOST=0.0.0.0\n","\n","# Run Flask app\n","CMD [\"python\", \"app.py\"]\n"],"metadata":{"id":"6UIbI8d9qmAe"},"id":"6UIbI8d9qmAe","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"yK1n7jBcRrYr","metadata":{"id":"yK1n7jBcRrYr"},"source":["## Setting up a Hugging Face Docker Space for the Backend"]},{"cell_type":"code","source":["# copy model to backend_files folder\n","!cp superkart_pipeline.pkl backend_files"],"metadata":{"id":"W7ZC-tEH2toK"},"id":"W7ZC-tEH2toK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","print(os.listdir('backend_files'))"],"metadata":{"id":"oqLAVa4J2YGm"},"id":"oqLAVa4J2YGm","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"B4tnVrlo8xQ9","metadata":{"id":"B4tnVrlo8xQ9"},"source":["## Uploading Files to Hugging Face Space (Docker Space)"]},{"cell_type":"code","source":["# for hugging face space authentication to upload files\n","from huggingface_hub import login, HfApi\n","\n","access_key = \"********************\"  # Hugging Face token created from access keys in write mode\n","repo_id = \"kke005012/super-kart-backend\"  # Hugging Face space id\n","\n","# Login to Hugging Face platform with the access token\n","login(token=access_key)\n","\n","# Initialize the API\n","api = HfApi()\n","\n","# Upload Streamlit app files stored in the folder called deployment_files\n","api.upload_folder(\n","    folder_path=\"backend_files\",  # Local folder path\n","    repo_id=repo_id,  # Hugging face space id\n","    repo_type=\"space\",  # Hugging face repo type \"space\"\n",")"],"metadata":{"id":"IYOJipQ3qmrD"},"id":"IYOJipQ3qmrD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json  # To handle JSON formatting for API requests and responses\n","import requests  # To send HTTP requests to the deployed Flask API\n","\n","import pandas as pd  # For data manipulation and analysis\n","import numpy as np  # For numerical computations\n","\n","from sklearn.model_selection import train_test_split  # To split data for batch inference scenarios"],"metadata":{"id":"GmkawePwGIgN"},"id":"GmkawePwGIgN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TEST BACKEND\n","\n","import requests\n","import pandas as pd\n","\n","# ----------------------------\n","# CONFIG\n","# ----------------------------\n","BACKEND_URL = \"https://kke005012-super-kart-backend.hf.space\"\n","SCHEMA_URL = f\"{BACKEND_URL}/schema\"\n","PREDICT_URL = f\"{BACKEND_URL}/predict\"\n","BATCH_PREDICT_URL = f\"{BACKEND_URL}/batch_predict\"\n","\n","CATEGORY_VALUES = {\n","    \"Store_Location_City_Type\": \"Tier 1\",\n","    \"Store_Type\": \"Supermarket\",\n","    \"Product_Type\": \"Electronics\",\n","    \"Product_Sugar_Content\": \"Regular\",\n","    \"Store_Size\": \"Medium\"\n","}\n","\n","sample_record = {\n","    \"Product_Id\": \"FD1781\",\n","    \"Product_Weight\": 12.53,\n","    \"Product_Sugar_Content\": \"Regular\",\n","    \"Product_Allocated_Area\": 0.066,\n","    \"Product_Type\": \"Snack Foods\",\n","    \"Product_MRP\": 145.62,\n","    \"Store_Id\": \"OUT004\",\n","    \"Store_Establishment_Year\": 2009,\n","    \"Store_Size\": \"Medium\",\n","    \"Store_Location_City_Type\": \"Tier 2\",\n","    \"Store_Type\": \"Supermarket Type2\"\n","}\n","\n","# ----------------------------\n","# Fetch schema\n","# ----------------------------\n","def get_schema():\n","    print(\"Fetching schema...\")\n","    resp = requests.get(SCHEMA_URL)\n","    if resp.status_code != 200:\n","        print(f\"Failed to fetch schema: {resp.status_code}, {resp.text}\")\n","        return None\n","    schema = resp.json()\n","    print(f\"Schema loaded: {schema['required_fields']}\")\n","    return schema\n","\n","\n","# ----------------------------\n","# Build sample record\n","# ----------------------------\n","def build_sample_record(required_fields):\n","    record = {}\n","    for field in required_fields:\n","        if any(keyword in field.lower() for keyword in [\"mrp\", \"weight\", \"area\"]):\n","            record[field] = 1.0\n","        elif field in CATEGORY_VALUES:\n","            record[field] = CATEGORY_VALUES[field]\n","        else:\n","            record[field] = \"valid_category\"\n","    return record\n","\n","\n","# ----------------------------\n","#  Enforce schema\n","# ----------------------------\n","def enforce_schema(df, required_fields):\n","    numeric_fields = [\n","        \"Product_Allocated_Area\",\n","        \"Product_MRP\",\n","        \"Product_Weight\"\n","    ]\n","\n","    for field in required_fields:\n","        if field not in df.columns:\n","            df[field] = None\n","\n","    for field in numeric_fields:\n","        if field in df.columns:\n","            df[field] = pd.to_numeric(df[field], errors=\"coerce\").fillna(0)\n","\n","    categorical_fields = [f for f in required_fields if f not in numeric_fields]\n","    for field in categorical_fields:\n","        if field in df.columns:\n","            df[field] = df[field].astype(str).fillna(\"missing\")\n","\n","    df = df[required_fields]\n","\n","    return df\n","\n","# ----------------------------\n","#  Test single prediction\n","# ----------------------------\n","def test_predict(record, required_fields):\n","    df = pd.DataFrame([record])\n","    df = enforce_schema(df, required_fields)\n","\n","    resp = requests.post(PREDICT_URL, json=df.iloc[0].to_dict())\n","    print(\"Status code:\", resp.status_code)\n","    print(\"Response:\", resp.json())\n","\n","# ----------------------------\n","#  Test batch prediction\n","# ----------------------------\n","def test_batch_predict(records, required_fields):\n","    df = pd.DataFrame(records)\n","    df = enforce_schema(df, required_fields)\n","\n","    resp = requests.post(BATCH_PREDICT_URL, json=df.to_dict(orient=\"records\"))\n","    print(\"Status code:\", resp.status_code)\n","    print(\"Response:\", resp.json())\n","\n","\n","# ----------------------------\n","# MAIN\n","# ----------------------------\n","def main():\n","    schema = get_schema()\n","    if not schema:\n","        return\n","\n","    required_fields = schema[\"required_fields\"]\n","\n","    record = build_sample_record(required_fields)\n","    test_predict(record, required_fields)\n","\n","    records = [build_sample_record(required_fields) for _ in range(2)]\n","    test_batch_predict(records, required_fields)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"ElkECLHcpYeW"},"id":"ElkECLHcpYeW","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"bv07DWg0_G6L","metadata":{"id":"bv07DWg0_G6L"},"source":["# **Deployment - Frontend**"]},{"cell_type":"markdown","source":["## Points to note before executing the below cells\n","- Create a Streamlit space on Hugging Face by following the instructions provided on the content page titled **`Creating Spaces and Adding Secrets in Hugging Face`** from Week 1"],"metadata":{"id":"3J1woYZNGhXh"},"id":"3J1woYZNGhXh"},{"cell_type":"markdown","id":"UsCYxkq_UL3Q","metadata":{"id":"UsCYxkq_UL3Q"},"source":["## Streamlit for Interactive UI"]},{"cell_type":"code","source":["# Create a folder for storing the files needed for frontend UI deployment\n","os.makedirs(\"frontend_files\", exist_ok=True)"],"metadata":{"id":"aC_Py-S9qpsJ"},"id":"aC_Py-S9qpsJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile frontend_files/app.py\n","import streamlit as st\n","import pandas as pd\n","import requests\n","\n","API_URL = \"https://kke005012-super-kart-backend.hf.space\"\n","\n","# -------------------------------\n","# Load schema on start\n","# -------------------------------\n","def load_schema():\n","    try:\n","        resp = requests.get(f\"{API_URL}/schema\", timeout=5)\n","        if resp.status_code == 200:\n","            return resp.json()\n","        else:\n","            st.error(f\"Schema request failed: {resp.status_code}\")\n","            return None\n","    except Exception as e:\n","        st.error(f\"Error loading schema: {e}\")\n","        return None\n","\n","schema = load_schema()\n","\n","if not schema:\n","    st.warning(\"Backend schema not loaded. Please wait until backend is ready.\")\n","    st.stop()\n","\n","required_fields = schema[\"required_fields\"]\n","example_record = schema[\"example_record\"]\n","\n","# -------------------------------\n","# UI\n","# -------------------------------\n","st.title(\"ðŸ›’ SuperKart Sales Predictor\")\n","\n","tab1, tab2 = st.tabs([\"Single Prediction\", \"Batch Prediction\"])\n","\n","with tab1:\n","    st.subheader(\"Single Prediction\")\n","\n","    # Dynamically generate inputs based on required_fields\n","    input_data = {}\n","    for field in required_fields:\n","        # Customize input type depending on field name or type\n","        if \"MRP\" in field or \"Weight\" in field or \"Area\" in field:\n","            input_data[field] = st.number_input(field, min_value=0.0, value=1.0, step=1.0)\n","        else:\n","            input_data[field] = st.text_input(field, value=\"value_here\")\n","\n","    if st.button(\"Predict Sales\"):\n","        try:\n","            payload = input_data\n","            response = requests.post(f\"{API_URL}/predict\", json=payload, timeout=10)\n","\n","            if response.status_code == 200:\n","                prediction = response.json()[\"prediction\"]\n","                st.success(f\"Predicted Store Sales: {prediction:,.2f}\")\n","            else:\n","                st.error(f\"Prediction failed: {response.text}\")\n","\n","        except Exception as e:\n","            st.error(f\"Error calling predict: {e}\")\n","\n","with tab2:\n","    st.subheader(\"Batch Prediction\")\n","\n","    uploaded_file = st.file_uploader(\"Upload CSV for batch prediction\", type=[\"csv\"])\n","    if uploaded_file:\n","        try:\n","            df = pd.read_csv(uploaded_file)\n","\n","            if st.button(\"Run Batch Prediction\"):\n","                records = df.to_dict(orient=\"records\")\n","                response = requests.post(f\"{API_URL}/batch_predict\", json=records, timeout=20)\n","\n","                if response.status_code == 200:\n","                    predictions = response.json()[\"predictions\"]\n","                    df[\"Prediction\"] = predictions\n","                    st.success(\"Batch predictions complete\")\n","                    st.dataframe(df)\n","                    st.download_button(\"Download Predictions\", df.to_csv(index=False), \"predictions.csv\")\n","                else:\n","                    st.error(f\"Batch prediction failed: {response.text}\")\n","\n","        except Exception as e:\n","            st.error(f\"Error processing file: {e}\")"],"metadata":{"id":"fMJWBTZI6Hlm"},"id":"fMJWBTZI6Hlm","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"beq1RbMhUQmi","metadata":{"id":"beq1RbMhUQmi"},"source":["## Dependencies File"]},{"cell_type":"code","source":["%%writefile frontend_files/requirements.txt\n","pandas==2.2.2\n","requests==2.28.1\n","streamlit==1.43.2"],"metadata":{"id":"3BcfxQ1VGjb6"},"id":"3BcfxQ1VGjb6","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"B-zE77eWcuGo","metadata":{"id":"B-zE77eWcuGo"},"source":["## DockerFile"]},{"cell_type":"code","source":["%%writefile frontend_files/Dockerfile\n","# Use a minimal base image with Python 3.9 installed\n","FROM python:3.9-slim\n","\n","# Set the working directory inside the container to /app\n","WORKDIR /app\n","\n","# Copy all files from the current directory on the host to the container's /app directory\n","COPY . .\n","\n","# Install Python dependencies listed in requirements.txt\n","RUN pip3 install -r requirements.txt\n","\n","# Define the command to run the Streamlit app on port 8501 and make it accessible externally\n","CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]\n","\n","# NOTE: Disable XSRF protection for easier external access in order to make batch predictions"],"metadata":{"id":"Tl5MzECZGufV"},"id":"Tl5MzECZGufV","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"5Re8ovwv9Rb5","metadata":{"id":"5Re8ovwv9Rb5"},"source":["## Uploading Files to Hugging Face Space (Streamlit Space)"]},{"cell_type":"code","source":["# for hugging face space authentication to upload files\n","from huggingface_hub import login, HfApi\n","\n","access_key = \"************************\"  # Hugging Face token created from access keys in write mode\n","repo_id = \"kke005012/super-kart-Sales-Forecasting\"  # Hugging Face space id\n","\n","# Login to Hugging Face platform with the access token\n","login(token=access_key)\n","\n","# Initialize the API\n","api = HfApi()\n","\n","# Upload Streamlit app files stored in the folder called deployment_files\n","api.upload_folder(\n","    folder_path=\"frontend_files\",  # Local folder path\n","    repo_id=repo_id,  # Hugging face space id\n","    repo_type=\"space\",  # Hugging face repo type \"space\"\n",")"],"metadata":{"id":"GKFHV8c0qs-l"},"id":"GKFHV8c0qs-l","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install streamlit"],"metadata":{"id":"Q0X6M95Eer6R"},"id":"Q0X6M95Eer6R","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TEST FRONTEND\n","\n","import requests\n","import pandas as pd\n","import streamlit as st\n","\n","# Backend Hugging Face URL\n","API_URL = \"https://kke005012-super-kart-backend.hf.space\"\n","\n","st.title(\"SuperKart Sales Predictor\")\n","\n","# Define tabs\n","tab1, tab2 = st.tabs([\"Single Prediction\", \"Batch Prediction\"])\n","\n","# -------------------------------\n","# Test /schema\n","# -------------------------------\n","print(\" Testing /schema...\")\n","resp = requests.get(f\"{API_URL}/schema\", timeout=10)\n","print(\"Status:\", resp.status_code)\n","print(\"Response:\", resp.json())\n","\n","# Save schema for later use\n","schema = resp.json()\n","required_fields = schema[\"required_fields\"]\n","\n","# -------------------------------\n","# Test /predict (single record)\n","# -------------------------------\n","print(\"\\n Testing /predict...\")\n","\n","# Build a sample payload based on required fields\n","sample_record = {}\n","for field in required_fields:\n","    if \"MRP\" in field or \"Weight\" in field or \"Area\" in field:\n","        sample_record[field] = 100.0  # numeric fields\n","    else:\n","        sample_record[field] = \"test_value\"  # categorical fields\n","\n","resp = requests.post(f\"{API_URL}/predict\", json=sample_record, timeout=10)\n","print(\"Status:\", resp.status_code)\n","print(\"Response:\", resp.json())\n","\n","# -------------------------------\n","# Test /batch_predict\n","# -------------------------------\n","print(\"\\n Testing /batch_predict...\")\n","\n","# Create two sample records for batch\n","batch_records = [\n","    sample_record,\n","    {**sample_record, \"Product_MRP\": 200.0, \"Product_Weight\": 10.5}\n","]\n","\n","resp = requests.post(f\"{API_URL}/batch_predict\", json=batch_records, timeout=15)\n","print(\"Status:\", resp.status_code)\n","print(\"Response:\", resp.json())"],"metadata":{"id":"qbxmNHycdLbh"},"id":"qbxmNHycdLbh","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"e4213339","metadata":{"id":"e4213339"},"source":["# **Actionable Insights and Business Recommendations**"]},{"cell_type":"markdown","source":["### Actionable Insights from Modeling\n","\n","* Product Pricing Impact\n","  - Product MRP is the strongest driver of sales. Optimizing pricing strategy (e.g., targeted discounts, dynamic pricing) can significantly influence revenue.\n","\n","* Store-Level Importance\n","  - Store attributes (location, size, sales area) still contribute meaningfully, showing operational or demographic differences across stores.\n","  - Some stores consistently underperform despite similar product MRPs, indicating localized marketing or supply chain issues.\n","\n","* Product Allocation Efficiency\n","\n","  - Product Allocated Area had outliers and showed moderate importance. Misaligned shelf space may reduce potential sales.\n","\n","* Model Performance\n","\n","  - Random Forest explained ~93% of variance in test data (Adj. RÂ² = 0.929), showing strong predictive ability. This indicates sales are predictable using the available features.\n","\n","  ### Business Recommendations\n","\n","Dynamic Pricing Strategy\n","\n","1. Leverage model predictions to adjust MRPs seasonally or per-store to maximize revenue.\n","   - Run simulations of price elasticity using the trained model before deploying promotions.\n","\n","2. Store-Specific Interventions\n","   - Identify underperforming stores and drill down into local drivers (competition, demographics, logistics).\n","   - Tailor marketing campaigns and inventory decisions based on store-specific sales forecasts.\n","\n","3. Shelf Space Optimization\n","   - Align product allocated area with sales impact. Increase visibility/shelf space for high-potential products to boost sales.\n","\n","4. Inventory & Demand Forecasting\n","   - Use the trained Random Forest model as part of a demand forecasting tool for supply chain planning.\n","   - Reduce stockouts and overstocking by predicting sales volume more accurately.\n","\n","5. Model Deployment for Decision Support\n","   - Deploy the trained model (via Flask API + Streamlit dashboard) to enable business users to test \"what-if\" scenarios (e.g., \"If we increase MRP by 5%, what happens to sales?\")."],"metadata":{"id":"gzTwDHFNZMjo"},"id":"gzTwDHFNZMjo"},{"cell_type":"code","source":["!jupyter nbconvert --to html --execute --template basic /content/drive/MyDrive/Full_Code_SuperKart_Model_Deployment_KristiEsta.ipynb"],"metadata":{"id":"1E8WJ1Z7UUVo"},"id":"1E8WJ1Z7UUVo","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["k-xVqOB6QTm6","AJDbTeirQasg","Aasy7LC_Qpq5","v-HxlIhTQ0-E","60VhOlydQ-PG","5fd3cabe","YyzOQ8pBY93N"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}